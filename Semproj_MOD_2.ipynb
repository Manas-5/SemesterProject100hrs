{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_GSW8KJRxq9k",
        "outputId": "221b36f6-5b38-4892-a2d0-9b03c61f8f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x data check: 0 0\n",
            "y data check: 0 0\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 1/500\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7632 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7480 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7324 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.7170 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7020 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6871 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6725 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6583 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6444 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6311 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.00017100000550271944.\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6174 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6063 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5950 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5830 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5719 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5605 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5490 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5374 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5251 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5140 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.00015390000626211986.\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5015 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4909 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.4798 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4687 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4552 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4436 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4340 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.4196 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4056 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3936 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 0.00013851000694558026.\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3780 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3646 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3507 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3380 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3202 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.3040 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2878 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2685 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2507 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2307 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 0.00012465900363167748.\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.2118 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.1940 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.1725 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1433 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.1183 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1027 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.0664 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0394 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0138 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.9704 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 0.00011219310981687158.\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.9477 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.9041 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.8655 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.8310 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.7925 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.7556 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.7238 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.6900 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6572 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.6245 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 61: LearningRateScheduler setting learning rate to 0.00010097380145452916.\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.5877 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 62: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.5600 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 63: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.5362 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 64: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.5100 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 65: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.4831 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 66: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.4607 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 67: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.4381 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 68: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.4158 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 69: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3927 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 70: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.3704 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 71: LearningRateScheduler setting learning rate to 9.087642392842099e-05.\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3506 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 72: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3319 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 73: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.3133 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 74: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2959 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 75: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2781 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 76: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2611 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 77: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2442 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 78: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2274 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 79: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2116 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 80: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1951 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 81: LearningRateScheduler setting learning rate to 8.178878415492364e-05.\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1791 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 82: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1653 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 83: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1515 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 84: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1377 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 85: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1241 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 86: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1108 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 87: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0976 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 88: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0846 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 89: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0716 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 90: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0590 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 91: LearningRateScheduler setting learning rate to 7.360990639426745e-05.\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0463 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 92: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0349 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 93: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0238 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 94: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0128 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 95: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0020 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 96: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9912 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 97: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9804 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 98: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9698 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 99: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.9593 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 100: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.9490 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 101: LearningRateScheduler setting learning rate to 6.624891248065979e-05.\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9385 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 102: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9294 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 103: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9201 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 104: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9112 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 105: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9021 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 106: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8932 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 107: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.8844 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 108: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.8755 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 109: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.8668 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 110: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8582 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 111: LearningRateScheduler setting learning rate to 5.962401992292144e-05.\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.8497 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 112: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8420 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 113: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.8344 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 114: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8268 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 115: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8193 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 116: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8117 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 117: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.8043 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 118: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.7970 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 119: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7896 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 120: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7823 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 121: LearningRateScheduler setting learning rate to 5.366161858546548e-05.\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7752 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 122: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7686 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 123: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7622 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 124: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7558 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 125: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7494 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 126: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.7430 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 127: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.7367 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 128: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7305 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 129: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7242 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 130: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.7180 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 131: LearningRateScheduler setting learning rate to 4.829545541724656e-05.\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.7118 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 132: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.7063 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 133: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7008 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 134: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6953 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 135: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6898 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 136: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6844 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 137: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6790 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 138: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6737 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 139: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6683 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 140: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6630 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 141: LearningRateScheduler setting learning rate to 4.346591085777618e-05.\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6576 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 142: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6529 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 143: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6481 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 144: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6434 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 145: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6387 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 146: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6340 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 147: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6293 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 148: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6247 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 149: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6201 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 150: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6155 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 151: LearningRateScheduler setting learning rate to 3.911932108167093e-05.\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6108 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 152: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6067 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 153: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6026 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 154: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5985 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 155: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5944 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 156: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5904 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 157: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5863 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 158: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5822 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 159: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5782 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 160: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5742 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 161: LearningRateScheduler setting learning rate to 3.520738864608575e-05.\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5702 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 162: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5666 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 163: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5630 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 164: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5595 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 165: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5559 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 166: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5523 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 167: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5488 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 168: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5452 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 169: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5418 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 170: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5383 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 171: LearningRateScheduler setting learning rate to 3.16866487992229e-05.\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5347 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 172: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5316 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 173: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5284 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 174: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5253 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 175: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5222 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 176: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5191 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 177: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5160 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 178: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5129 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 179: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5098 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 180: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5067 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 181: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5036 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 182: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5009 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 183: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4981 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 184: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4953 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 185: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4926 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 186: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4899 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 187: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4872 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 188: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4844 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 189: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.4817 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 190: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4790 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 191: LearningRateScheduler setting learning rate to 2.5666186411399396e-05.\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4763 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 192: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4738 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 193: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4715 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 194: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4690 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 195: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4666 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 196: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4642 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 197: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4618 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 198: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4594 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 199: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4570 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 200: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4546 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 201: LearningRateScheduler setting learning rate to 2.3099567442841362e-05.\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4522 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 202: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4500 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 203: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4479 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 204: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4458 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 205: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4436 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 206: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4415 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 207: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4393 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 208: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4372 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 209: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4351 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 210: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4330 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 211: LearningRateScheduler setting learning rate to 2.078961151710246e-05.\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4308 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 212: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4289 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 213: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4270 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 214: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4251 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 215: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4233 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 216: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4214 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 217: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4195 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 218: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4176 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 219: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4157 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 220: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4138 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 221: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4119 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 222: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4102 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 223: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4086 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 224: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4069 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 225: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4052 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 226: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4035 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 227: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4019 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 228: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4002 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 229: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3985 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 230: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3969 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 231: LearningRateScheduler setting learning rate to 1.6839585623529273e-05.\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3952 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 232: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3937 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 233: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3922 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 234: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3907 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 235: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3892 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 236: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3877 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 237: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3862 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 238: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3847 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 239: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3833 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 240: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3817 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 241: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3803 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 242: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3789 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 243: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3776 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 244: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3763 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 245: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3749 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 246: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3736 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 247: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3723 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 248: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3709 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 249: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3696 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 250: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3683 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 251: LearningRateScheduler setting learning rate to 1.3640064207720571e-05.\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3669 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 252: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3658 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 253: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3646 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 254: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3634 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 255: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3622 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 256: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3610 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 257: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3599 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 258: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3587 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 259: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3575 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 260: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3563 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 261: LearningRateScheduler setting learning rate to 1.2276057623239467e-05.\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3551 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 262: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3541 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 263: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3530 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 264: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3519 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 265: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3509 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 266: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3498 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 267: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3488 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 268: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3477 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 269: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3467 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 270: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3456 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 271: LearningRateScheduler setting learning rate to 1.1048451779060998e-05.\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3446 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 272: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3436 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 273: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3427 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 274: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3417 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 275: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3408 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 276: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3398 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 277: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3389 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 278: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3379 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 279: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3370 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 280: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3361 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 281: LearningRateScheduler setting learning rate to 9.943606437445851e-06.\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3351 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 282: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3343 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 283: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3334 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 284: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3326 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 285: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3318 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 286: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3309 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 287: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3300 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 288: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3292 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 289: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3283 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 290: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3275 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 291: LearningRateScheduler setting learning rate to 8.94924587555579e-06.\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3267 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 292: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3259 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 293: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3252 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 294: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3244 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 295: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3236 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 296: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3229 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 297: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3221 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 298: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3214 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 299: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3206 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 300: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3199 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 301: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3191 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 302: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3184 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 303: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3177 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 304: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3170 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 305: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3164 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 306: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3157 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 307: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3150 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 308: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3144 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 309: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3137 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 310: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3130 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 311: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3124 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 312: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3117 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 313: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3111 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 314: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3105 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 315: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3099 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 316: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3093 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 317: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3087 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 318: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3081 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 319: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3075 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 320: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3069 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 321: LearningRateScheduler setting learning rate to 6.524000309582334e-06.\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3063 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 322: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3057 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 323: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3052 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 324: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3047 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 325: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3041 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 326: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3035 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 327: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3030 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 328: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3025 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 329: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3019 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 330: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3014 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 331: LearningRateScheduler setting learning rate to 5.871600114915055e-06.\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3008 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 332: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3004 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 333: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2998 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 334: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2994 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 335: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2989 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 336: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2984 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 337: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2979 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 338: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2974 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 339: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2970 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 340: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2964 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 341: LearningRateScheduler setting learning rate to 5.284440021569026e-06.\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2960 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 342: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2955 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 343: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2951 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 344: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2947 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 345: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2942 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 346: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2938 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 347: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2933 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 348: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2929 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 349: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2925 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 350: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2920 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 351: LearningRateScheduler setting learning rate to 4.755995814775815e-06.\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2916 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 352: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2912 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 353: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2908 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 354: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2904 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 355: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2900 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 356: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2896 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 357: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2892 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 358: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2889 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 359: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2885 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 360: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2881 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 361: LearningRateScheduler setting learning rate to 4.280396069589187e-06.\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2877 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 362: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2873 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 363: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2870 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 364: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2866 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 365: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2863 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 366: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2859 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 367: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2855 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 368: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2852 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 369: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2849 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 370: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2845 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 371: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2842 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 372: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2838 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 373: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2835 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 374: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2832 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 375: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2829 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 376: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2826 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 377: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2823 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 378: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2819 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 379: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2816 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 380: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2813 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 381: LearningRateScheduler setting learning rate to 3.467120632194565e-06.\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2810 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 382: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2807 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 383: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2804 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 384: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2801 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 385: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2799 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 386: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2796 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 387: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2793 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 388: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2790 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 389: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2787 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 390: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2785 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 391: LearningRateScheduler setting learning rate to 3.12040860990237e-06.\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2782 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 392: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2779 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 393: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2777 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 394: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2774 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 395: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2772 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 396: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2769 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 397: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2766 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 398: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2764 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 399: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2761 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 400: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2759 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 401: LearningRateScheduler setting learning rate to 2.8083677079848714e-06.\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2756 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 402: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2754 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 403: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2752 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 404: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2749 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 405: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2747 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 406: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2745 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 407: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2743 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 408: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2740 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 409: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2738 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 410: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2736 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 411: LearningRateScheduler setting learning rate to 2.527530978113646e-06.\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2733 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 412: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2731 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 413: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2729 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 414: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2727 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 415: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2725 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 416: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2723 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 417: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2721 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 418: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2719 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 419: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2717 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 420: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2715 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 421: LearningRateScheduler setting learning rate to 2.2747779212295426e-06.\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2713 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 422: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2711 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 423: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2709 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 424: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2707 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 425: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2705 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 426: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2704 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 427: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2702 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 428: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2700 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 429: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2698 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 430: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2696 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 431: LearningRateScheduler setting learning rate to 2.0473001086429576e-06.\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2694 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 432: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2692 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 433: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2691 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 434: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2689 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 435: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2688 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 436: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2686 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 437: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2684 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 438: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2683 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 439: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2681 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 440: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2679 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 441: LearningRateScheduler setting learning rate to 1.8425700773150312e-06.\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2678 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 442: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2676 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 443: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2675 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 444: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2673 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 445: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2672 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 446: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2670 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 447: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2669 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 448: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2667 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 449: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2665 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 450: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2664 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 451: LearningRateScheduler setting learning rate to 1.6583130900471589e-06.\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2663 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 452: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2662 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 453: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2660 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 454: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2659 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 455: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2657 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 456: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2656 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 457: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2655 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 458: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2654 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 459: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2652 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 460: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2651 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 461: LearningRateScheduler setting learning rate to 1.4924817605788121e-06.\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2650 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 462: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2648 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 463: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2647 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 464: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2646 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 465: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2645 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 466: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2643 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 467: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2642 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 468: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2641 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 469: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2640 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 470: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2639 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 471: LearningRateScheduler setting learning rate to 1.3432335435936694e-06.\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2638 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 472: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2636 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 473: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2635 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 474: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2634 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 475: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2633 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 476: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2632 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 477: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2631 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 478: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2630 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 479: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2629 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 480: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2628 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 481: LearningRateScheduler setting learning rate to 1.2089101687706717e-06.\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2626 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 482: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2625 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 483: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2625 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 484: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2624 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 485: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2623 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 486: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2622 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 487: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2621 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 488: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2620 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 489: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2619 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 490: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2618 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 491: LearningRateScheduler setting learning rate to 1.0880191211981583e-06.\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2617 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 492: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2616 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 493: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2615 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 494: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2614 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 495: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2613 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 496: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2613 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 497: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2612 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 498: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2611 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 499: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2610 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 500: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2609 - lr: 1.0880e-06\n",
            "Skipping update for di_bits_cap_est_metrics, expected 4 values but got 100\n",
            "Skipping update for di_bits_cap_est_metrics, expected 4 values but got 100\n",
            "Skipping update for di_bits_cap_est_metrics, expected 4 values but got 100\n",
            "Monte Carlo Evaluation - Mean Loss: 7.15375065851731e-08, Std Loss: 6.109049897895513e-12\n",
            "MDP Evaluation - Average Reward: [[[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]]\n",
            "Monte Carlo Evaluation - Mean Loss: 7.153763448286554e-08, Std Loss: 0.0\n",
            "MDP Evaluation - Average Reward: [[[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x600 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import logging\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logger = logging.getLogger(\"logger\")\n",
        "\n",
        "class ModelMetrics(tf.keras.metrics.Metric):\n",
        "    def __init__(self, writer, metric_name='', **kwargs):\n",
        "        super(ModelMetrics, self).__init__(name=metric_name, **kwargs)\n",
        "        self.writer = writer\n",
        "        self.metric_pool = [\n",
        "            DV(metric_name='dv_xy_{}'.format(metric_name)),\n",
        "            DV(metric_name='dv_y_{}'.format(metric_name)),\n",
        "            DI(metric_name='di_{}'.format(metric_name)),\n",
        "            PMF(metric_name='p_mean'),\n",
        "            DI_bits(metric_name='di_bits')\n",
        "        ]\n",
        "\n",
        "    def update_state(self, t_y, t_xy, **kwargs):\n",
        "        t_y = [tf.convert_to_tensor(t, dtype=tf.float32) for t in t_y]\n",
        "        t_xy = [tf.convert_to_tensor(t, dtype=tf.float32) for t in t_xy]\n",
        "\n",
        "        if not isinstance(t_y, (list, tuple)) or len(t_y) < 2:\n",
        "            raise ValueError(\"Invalid t_y format. Expected lists with at least two elements.\")\n",
        "        if not isinstance(t_xy, (list, tuple)) or len(t_xy) < 2:\n",
        "            raise ValueError(\"Invalid t_xy format. Expected lists with at least two elements.\")\n",
        "\n",
        "        self.metric_pool[0].update_state(t_xy[0], t_xy[1])\n",
        "        self.metric_pool[1].update_state(t_y[0], t_y[1])\n",
        "        self.metric_pool[2].update_state(t_y[0], t_y[1], t_xy[0], t_xy[1])\n",
        "        self.metric_pool[3].update_state(t_y[0], t_y[1], t_xy[0], t_xy[1])\n",
        "\n",
        "    def result(self):\n",
        "        return [metric.result() for metric in self.metric_pool]\n",
        "\n",
        "    def reset_states(self):\n",
        "        for metric in self.metric_pool:\n",
        "            metric.reset_states()\n",
        "        return\n",
        "\n",
        "    def log_metrics(self, epoch, model_name):\n",
        "        with self.writer.as_default():\n",
        "            for metric in self.metric_pool:\n",
        "                tf.summary.scalar(metric.name, metric.result(), epoch)\n",
        "\n",
        "        msg = [\"{} Epoch: {:05d}\\t\".format(self.name, epoch)]\n",
        "        for metric in self.metric_pool:\n",
        "            result = metric.result()\n",
        "            if np.isnan(result):\n",
        "                print(f\"NaN detected in metric {metric.name} during epoch {epoch}\")\n",
        "                continue\n",
        "            msg.append(\"{:s} {:3.6f}\\t\".format(metric.name, float(result)))\n",
        "        msg.append(model_name)\n",
        "        logger.info(\"\\t\".join(msg))\n",
        "\n",
        "class DV(tf.keras.metrics.Metric):  # estimated DV loss calculation metric class\n",
        "    def __init__(self, metric_name='dv_loss', **kwargs):\n",
        "        super(DV, self).__init__(name=metric_name, **kwargs)\n",
        "        self.T = self.add_weight(name='t', initializer='zeros')\n",
        "        self.exp_T_bar = self.add_weight(name='exp_t_bar', initializer='zeros')\n",
        "        self.global_counter = self.add_weight(name='n', initializer='zeros')\n",
        "        self.global_counter_ref = self.add_weight(name='n_ref', initializer='zeros')\n",
        "\n",
        "    def update_state(self, T, T_bar, **kwargs):\n",
        "        self.T.assign_add(tf.reduce_sum(T))\n",
        "        self.exp_T_bar.assign_add(tf.reduce_sum(T_bar))\n",
        "        self.global_counter.assign_add(tf.cast(tf.reduce_prod(T.shape[:-1]), dtype=tf.float32))\n",
        "        self.global_counter_ref.assign_add(tf.cast(tf.reduce_prod(T_bar.shape[:-1]), dtype=tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        loss = self.T / (self.global_counter + 1e-10) - K.log(self.exp_T_bar / (self.global_counter_ref + 1e-10))\n",
        "        return loss\n",
        "\n",
        "class DI(tf.keras.metrics.Metric):  # estimated DI calculation metric class\n",
        "    def __init__(self, metric_name='di_loss', **kwargs):\n",
        "        super(DI, self).__init__(name=metric_name, **kwargs)\n",
        "        self.c_T = self.add_weight(name='c_t', initializer='zeros')\n",
        "        self.c_exp_T_bar = self.add_weight(name='c_exp_t_bar', initializer='zeros')\n",
        "        self.xc_T = self.add_weight(name='xc_t', initializer='zeros')\n",
        "        self.xc_exp_T_bar = self.add_weight(name='xc_exp_t_bar', initializer='zeros')\n",
        "        self.global_counter = self.add_weight(name='n', initializer='zeros')\n",
        "        self.global_counter_ref = self.add_weight(name='n_ref', initializer='zeros')\n",
        "\n",
        "    def update_state(self, c_T, c_T_bar, xc_T, xc_T_bar, **kwargs):\n",
        "        self.c_T.assign_add(tf.reduce_sum(c_T))\n",
        "        self.c_exp_T_bar.assign_add(tf.reduce_sum(c_T_bar))\n",
        "\n",
        "        self.xc_T.assign_add(tf.reduce_sum(xc_T))\n",
        "        self.xc_exp_T_bar.assign_add(tf.reduce_sum(xc_T_bar))\n",
        "\n",
        "        self.global_counter.assign_add(tf.cast(tf.reduce_prod(c_T.shape[:-1]), dtype=tf.float32))\n",
        "        self.global_counter_ref.assign_add(tf.cast(tf.reduce_prod(c_T_bar.shape[:-1]), dtype=tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        loss_y = self.c_T / (self.global_counter + 1e-10) - K.log(self.c_exp_T_bar / (self.global_counter_ref + 1e-10))\n",
        "        loss_xy = self.xc_T / (self.global_counter + 1e-10) - K.log(self.xc_exp_T_bar / (self.global_counter_ref + 1e-10))\n",
        "        return loss_xy - loss_y\n",
        "\n",
        "class DI_bits(tf.keras.metrics.Metric):  # estimated DI calculation metric class in bits\n",
        "    def __init__(self, metric_name='di_bits_loss', **kwargs):\n",
        "        super(DI_bits, self).__init__(name=metric_name, **kwargs)\n",
        "        self.c_T = self.add_weight(name='c_t', initializer='zeros')\n",
        "        self.c_exp_T_bar = self.add_weight(name='c_exp_t_bar', initializer='zeros')\n",
        "        self.xc_T = self.add_weight(name='xc_t', initializer='zeros')\n",
        "        self.xc_exp_T_bar = self.add_weight(name='xc_exp_t_bar', initializer='zeros')\n",
        "        self.global_counter = self.add_weight(name='n', initializer='zeros')\n",
        "        self.global_counter_ref = self.add_weight(name='n_ref', initializer='zeros')\n",
        "\n",
        "    def update_state(self, c_T, c_T_bar, xc_T, xc_T_bar, **kwargs):\n",
        "        self.c_T.assign_add(tf.reduce_sum(c_T))\n",
        "        self.c_exp_T_bar.assign_add(tf.reduce_sum(c_T_bar))\n",
        "\n",
        "        self.xc_T.assign_add(tf.reduce_sum(xc_T))\n",
        "        self.xc_exp_T_bar.assign_add(tf.reduce_sum(xc_T_bar))\n",
        "\n",
        "        self.global_counter.assign_add(tf.cast(tf.reduce_prod(c_T.shape[:-1]), dtype=tf.float32))\n",
        "        self.global_counter_ref.assign_add(tf.cast(tf.reduce_prod(c_T_bar.shape[:-1]), dtype=tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        loss_y = self.c_T / (self.global_counter + 1e-10) - K.log(self.c_exp_T_bar / (self.global_counter_ref + 1e-10))\n",
        "        loss_xy = self.xc_T / (self.global_counter + 1e-10) - K.log(self.xc_exp_T_bar / (self.global_counter_ref + 1e-10))\n",
        "        return (loss_xy - loss_y) / math.log(2)\n",
        "\n",
        "class PMF(tf.keras.metrics.Metric):  # estimated DV loss calculation metric class\n",
        "    def __init__(self, metric_name='p_mean', **kwargs):\n",
        "        super(PMF, self).__init__(name=metric_name, **kwargs)\n",
        "        self.p = self.add_weight(name='p', initializer='zeros')\n",
        "        self.global_counter = self.add_weight(name='n', initializer='zeros')\n",
        "\n",
        "    def update_state(self, p, **kwargs):\n",
        "        self.p.assign_add(tf.reduce_sum(p))\n",
        "        self.global_counter.assign_add(tf.cast(tf.reduce_prod(p.shape[:-1]), dtype=tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        result = self.p / (self.global_counter + 1e-10)\n",
        "        return result\n",
        "\n",
        "# class ModelWithEncMetrics(ModelMetrics):\n",
        "#     def __init__(self, writer, metric_name='ModelWithEncMetrics'):\n",
        "#         self.writer = writer\n",
        "#         self.metric_name = metric_name\n",
        "#         self.metric_pool = [\n",
        "#             tf.keras.metrics.Mean(name='capacity_estimate'),\n",
        "#             tf.keras.metrics.Mean(name='info_rate'),\n",
        "#             CustomMetric(name='di_bits_cap_est_metrics')\n",
        "#         ]\n",
        "\n",
        "#     def update_state(self, values):\n",
        "#         for metric, value in zip(self.metric_pool, values):\n",
        "#             if tf.reduce_any(tf.math.is_nan(value)):\n",
        "#                 print(f\"NaN detected in value for metric {metric.name}\")\n",
        "#                 continue\n",
        "#             metric.update_state(value)\n",
        "\n",
        "#     def result(self):\n",
        "#         return [metric.result() for metric in self.metric_pool]\n",
        "\n",
        "#     def reset_states(self):\n",
        "#         for metric in self.metric_pool:\n",
        "#             metric.reset_states()\n",
        "\n",
        "#     def log_metrics(self, epoch, model_name):\n",
        "#         with self.writer.as_default():\n",
        "#             for metric in self.metric_pool:\n",
        "#                 result = metric.result()\n",
        "#                 if tf.math.is_nan(result):\n",
        "#                     print(f\"NaN detected in metric {metric.name} during epoch {epoch}\")\n",
        "#                     continue\n",
        "#                 tf.summary.scalar(metric.name, result, epoch)\n",
        "#         msg = [\"{} Epoch: {:05d}\\t\".format(self.metric_name, epoch)]\n",
        "#         for metric in self.metric_pool:\n",
        "#             result = metric.result()\n",
        "#             if tf.math.is_nan(result):\n",
        "#                 print(f\"NaN detected in metric {metric.name} during epoch {epoch}\")\n",
        "#                 continue\n",
        "#             msg.append(\"{:s} {:3.6f}\\t\".format(metric.name, float(result)))\n",
        "#         msg.append(model_name)\n",
        "#         logger.info(\"\\t\".join(msg))\n",
        "\n",
        "# class CustomMetric(tf.keras.metrics.Metric):\n",
        "#     def __init__(self, metric_name='custom_metric', **kwargs):\n",
        "#         # Remove name from kwargs if it exists\n",
        "#         if 'name' in kwargs:\n",
        "#             del kwargs['name']\n",
        "#         super(CustomMetric, self).__init__(name=metric_name, **kwargs)\n",
        "#         self.total = self.add_weight(name='total', initializer='zeros')\n",
        "#         self.count = self.add_weight(name='count', initializer='zeros')\n",
        "\n",
        "#     def update_state(self, c_T, c_T_bar, xc_T, xc_T_bar):\n",
        "#         # Ensure that NaNs are not introduced in the calculation\n",
        "#         if tf.reduce_any(tf.math.is_nan(c_T)) or tf.reduce_any(tf.math.is_nan(c_T_bar)) or \\\n",
        "#            tf.reduce_any(tf.math.is_nan(xc_T)) or tf.reduce_any(tf.math.is_nan(xc_T_bar)):\n",
        "#             print(\"NaN detected in inputs of custom metric update_state\")\n",
        "#             return\n",
        "\n",
        "#         # Custom logic for updating state\n",
        "#         values = (c_T + c_T_bar + xc_T + xc_T_bar) / 4\n",
        "#         self.total.assign_add(tf.reduce_sum(values))\n",
        "#         self.count.assign_add(tf.cast(tf.size(values), tf.float32))\n",
        "\n",
        "#     def result(self):\n",
        "#         return tf.math.divide_no_nan(self.total, self.count)\n",
        "\n",
        "#     def reset_states(self):\n",
        "#         self.total.assign(0.0)\n",
        "#         self.count.assign(0.0)\n",
        "\n",
        "class ModelWithEncMetrics(tf.keras.metrics.Metric):\n",
        "    def __init__(self, writer, metric_name='ModelWithEncMetrics', **kwargs):\n",
        "        super(ModelWithEncMetrics, self).__init__(name=metric_name, **kwargs)\n",
        "        self.writer = writer\n",
        "        self.metric_pool = [\n",
        "            tf.keras.metrics.Mean(name='capacity_estimate'),\n",
        "            tf.keras.metrics.Mean(name='info_rate'),\n",
        "            CustomMetric(name='di_bits_cap_est_metrics')\n",
        "        ]\n",
        "\n",
        "    def update_state(self, values, **kwargs):\n",
        "        # Ensure the correct number of arguments are passed for each metric\n",
        "        for metric, value in zip(self.metric_pool, values):\n",
        "            if isinstance(metric, CustomMetric):\n",
        "                if len(value) != 4:\n",
        "                    print(f\"Skipping update for {metric.name}, expected 4 values but got {len(value)}\")\n",
        "                    continue\n",
        "                metric.update_state(*value)\n",
        "            else:\n",
        "                metric.update_state(value)\n",
        "\n",
        "    def result(self):\n",
        "        return [metric.result() for metric in self.metric_pool]\n",
        "\n",
        "    def reset_states(self):\n",
        "        for metric in self.metric_pool:\n",
        "            metric.reset_states()\n",
        "\n",
        "    def log_metrics(self, epoch, model_name):\n",
        "        with self.writer.as_default():\n",
        "            for metric in self.metric_pool:\n",
        "                result = metric.result()\n",
        "                if tf.math.is_nan(result):\n",
        "                    print(f\"NaN detected in metric {metric.name} during epoch {epoch}\")\n",
        "                    continue\n",
        "                tf.summary.scalar(metric.name, result, epoch)\n",
        "        msg = [\"{} Epoch: {:05d}\\t\".format(self.name, epoch)]\n",
        "        for metric in self.metric_pool:\n",
        "            result = metric.result()\n",
        "            if tf.math.is_nan(result):\n",
        "                print(f\"NaN detected in metric {metric.name} during epoch {epoch}\")\n",
        "                continue\n",
        "            msg.append(\"{:s} {:3.6f}\\t\".format(metric.name, float(result)))\n",
        "        msg.append(model_name)\n",
        "        logger.info(\"\\t\".join(msg))\n",
        "\n",
        "\n",
        "class CustomMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='custom_metric', **kwargs):\n",
        "        # Remove name from kwargs if it exists\n",
        "        if 'name' in kwargs:\n",
        "            del kwargs['name']\n",
        "        super(CustomMetric, self).__init__(name=name, **kwargs)\n",
        "        self.total = self.add_weight(name='total', initializer='zeros')\n",
        "        self.count = self.add_weight(name='count', initializer='zeros')\n",
        "\n",
        "    def update_state(self, c_T, c_T_bar, xc_T, xc_T_bar, **kwargs):\n",
        "        # Ensure that NaNs are not introduced in the calculation\n",
        "        if tf.reduce_any(tf.math.is_nan(c_T)) or tf.reduce_any(tf.math.is_nan(c_T_bar)) or \\\n",
        "           tf.reduce_any(tf.math.is_nan(xc_T)) or tf.reduce_any(tf.math.is_nan(xc_T_bar)):\n",
        "            print(\"NaN detected in inputs of custom metric update_state\")\n",
        "            return tf.constant(0.0)  # Ensure a return value even in the case of NaN detection\n",
        "\n",
        "        # Custom logic for updating state\n",
        "        values = (c_T + c_T_bar + xc_T + xc_T_bar) / 4\n",
        "        self.total.assign_add(tf.reduce_sum(values))\n",
        "        self.count.assign_add(tf.cast(tf.size(values), tf.float32))\n",
        "        return tf.constant(1.0)  # Ensure a return value\n",
        "\n",
        "    def result(self):\n",
        "        return tf.math.divide_no_nan(self.total, self.count)\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.total.assign(0.0)\n",
        "        self.count.assign(0.0)\n",
        "\n",
        "\n",
        "\n",
        "class CapEstDI(object):\n",
        "    def __init__(self, model, data, config):\n",
        "        self.model = model\n",
        "        self.data_iterators = data\n",
        "        self.config = config\n",
        "        self.loss_fn = tf.keras.losses.Huber()\n",
        "        self.learning_rate = config['lr']\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        self.metrics = ModelWithEncMetrics(tf.summary.create_file_writer(config['tensor_board_dir']), metric_name='cap_est_metrics')\n",
        "        self.feedback = (config['feedback'] == 1)\n",
        "        self.T = config['T']\n",
        "        self.capacity_estimates = []\n",
        "        self.dine_estimates = []\n",
        "        self.info_rates = []\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        self.metrics.reset_states()\n",
        "        self.reset_model_states()\n",
        "\n",
        "        for _ in range(self.config['batches']):\n",
        "            x, y = self.data_iterators.gen_data()\n",
        "            x_y_combined = tf.concat([x, y], axis=-1)\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = self.model(x_y_combined, training=True)\n",
        "                loss = self.loss_fn(y, predictions)\n",
        "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "            c_T = predictions\n",
        "            c_T_bar = tf.math.exp(predictions)\n",
        "            xc_T = predictions * y\n",
        "            xc_T_bar = tf.math.exp(predictions * y)\n",
        "\n",
        "            # Debug print statements to check intermediate values\n",
        "            # print(f\"c_T: {c_T.numpy()[:5]}, c_T_bar: {c_T_bar.numpy()[:5]}\")\n",
        "            # print(f\"xc_T: {xc_T.numpy()[:5]}, xc_T_bar: {xc_T_bar.numpy()[:5]}\")\n",
        "\n",
        "        self.metrics.metric_pool[2].update_state(c_T, c_T_bar, xc_T, xc_T_bar)\n",
        "\n",
        "        self.metrics.log_metrics(epoch, model_name=\"Training\")\n",
        "\n",
        "        # Store estimates\n",
        "        self.capacity_estimates.append(self.metrics.metric_pool[0].result().numpy())\n",
        "        self.dine_estimates.append(self.metrics.metric_pool[2].result().numpy())\n",
        "        self.info_rates.append(self.metrics.metric_pool[1].result().numpy())\n",
        "\n",
        "    def evaluate(self, epoch):\n",
        "        self.metrics.reset_states()\n",
        "        x, y = self.data_iterators.gen_data()\n",
        "        x_y_combined = tf.concat([x, y], axis=-1)\n",
        "        predictions = self.model(x_y_combined, training=False)\n",
        "        loss = self.loss_fn(y, predictions)\n",
        "        self.metrics.update_state([loss, loss, tf.random.uniform(shape=[self.config['batch_size'], 1], minval=0, maxval=1)])\n",
        "        self.metrics.log_metrics(epoch, model_name=\"Evaluation\")\n",
        "\n",
        "    def train(self, num_epochs=None):\n",
        "        if num_epochs is None:\n",
        "            num_epochs = self.config.get('num_epochs', 500)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            if epoch % self.config['eval_freq'] == 0:\n",
        "                self.evaluate(epoch)\n",
        "            self.train_epoch(epoch)\n",
        "\n",
        "        self.evaluate(num_epochs)\n",
        "\n",
        "    def reset_model_states(self):\n",
        "        for layer in self.model.layers:\n",
        "            if hasattr(layer, 'reset_states') and layer.stateful:\n",
        "                layer.reset_states()\n",
        "\n",
        "    def mc_evaluation(self, num_simulations=1000):\n",
        "        results = []\n",
        "        for _ in range(num_simulations):\n",
        "            x, y = self.data_iterators.gen_data()\n",
        "            x_y_combined = tf.concat([x, y], axis=-1)\n",
        "            predictions = self.model(x_y_combined, training=False)\n",
        "            loss = self.loss_fn(y, predictions)\n",
        "            results.append(loss.numpy())\n",
        "        return np.mean(results), np.std(results)\n",
        "\n",
        "    def mdp_evaluation(self, policy, num_steps=100):\n",
        "        state = self.data_iterators.gen_data()[0]\n",
        "        total_reward = 0\n",
        "        for _ in range(num_steps):\n",
        "            action = policy(state)\n",
        "            next_state, reward = self.data_iterators.gen_data()\n",
        "            total_reward += reward.numpy()\n",
        "            state = next_state\n",
        "        return total_reward / num_steps\n",
        "\n",
        "class Ising_Data(object):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.p_x = 0.4503\n",
        "        self.p_ch = 0.5\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.bptt = config['bptt']\n",
        "        self.ising_ch_logits = self.gen_logits(self.p_ch)\n",
        "        self.ising_x_logits = self.gen_logits(1 - self.p_x)\n",
        "        self.initialize_channel()\n",
        "\n",
        "    def gen_logits(self, p):\n",
        "        p_t = p * tf.ones(shape=[self.batch_size, 1], dtype=tf.float32)\n",
        "        p_bar_t = tf.ones_like(p_t) - p_t\n",
        "        logits = tf.math.log(tf.concat([p_bar_t, p_t], axis=1) + 1e-10)\n",
        "        return logits\n",
        "\n",
        "    def gen_data(self):\n",
        "        y_l = []\n",
        "        x_l = []\n",
        "        for t in range(self.bptt):\n",
        "            self.encoder()\n",
        "            x_l.append(self.x)\n",
        "            self.channel()\n",
        "            y_l.append(self.y)\n",
        "        x = tf.concat(x_l, axis=1)\n",
        "        y = tf.concat(y_l, axis=1)\n",
        "        return x, y\n",
        "\n",
        "    def initialize_channel(self):\n",
        "        for step in [0, 1]:\n",
        "            self.encoder(step)\n",
        "            self.channel(step)\n",
        "\n",
        "    def encoder(self, step=None):\n",
        "        if step == 0:\n",
        "            self.s_past = tf.zeros(shape=[self.batch_size, 1, 1], dtype=tf.float32)\n",
        "            self.x = tf.cast(tf.random.categorical(logits=self.ising_x_logits, num_samples=1), dtype=tf.float32)\n",
        "            self.x = tf.expand_dims(self.x, axis=-1)\n",
        "        elif step == 1:\n",
        "            self.x = self.x\n",
        "        else:\n",
        "            z = tf.cast(tf.random.categorical(logits=self.ising_x_logits, num_samples=1), dtype=tf.float32)\n",
        "            z = tf.expand_dims(z, axis=-1)\n",
        "            x_p = tf.math.floormod(self.x + z, 2)\n",
        "            x_new = tf.where(tf.equal(self.y, self.s_past), self.s, x_p)\n",
        "            self.x = x_new\n",
        "\n",
        "    def channel(self, step=None):\n",
        "        z = tf.cast(tf.random.categorical(logits=self.ising_ch_logits, num_samples=1), dtype=tf.float32)\n",
        "        z = tf.expand_dims(z, axis=-1)\n",
        "        if step == 0:\n",
        "            self.s = self.s_past\n",
        "        self.y = tf.where(tf.equal(z, 0), self.x, self.s)\n",
        "        self.s_past = self.s\n",
        "        self.s = self.x\n",
        "\n",
        "def complex_lstm_model_v3(input_shape, config):\n",
        "    randN_05 = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
        "    bias_init = tf.keras.initializers.Constant(0.01)\n",
        "\n",
        "    if config['compression_flag'] == 1:\n",
        "        DV_hidden = config['hidden_size_compression']\n",
        "    else:\n",
        "        DV_hidden = config['hidden_size']\n",
        "\n",
        "    max_norm = config['max_norm_y']\n",
        "\n",
        "    inputs = tf.keras.Input(shape=input_shape, batch_size=config['batch_size'])\n",
        "    x = tf.keras.layers.LSTM(DV_hidden[0], return_sequences=True, stateful=True, dropout=config['dropout'], recurrent_dropout=config['dropout'])(inputs)\n",
        "    x = tf.keras.layers.Dense(DV_hidden[1], activation=\"relu\", kernel_initializer=randN_05, bias_initializer=bias_init)(x)\n",
        "    x = tf.keras.layers.Dense(DV_hidden[2], activation=\"relu\", kernel_initializer=randN_05, bias_initializer=bias_init)(x)\n",
        "    outputs = tf.keras.layers.Dense(1, activation=config['output_activation'], kernel_initializer=randN_05, bias_initializer=bias_init, kernel_constraint=tf.keras.constraints.MaxNorm(max_value=max_norm))(x)\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "class LossHistory(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.lr = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.lr.append(self.model.optimizer.learning_rate.numpy())\n",
        "\n",
        "def negative_log_likelihood(y_true, y_pred):\n",
        "    return -tf.reduce_mean(tf.reduce_sum(y_true * tf.math.log(y_pred + 1e-10), axis=-1))\n",
        "\n",
        "# Configuration\n",
        "config = {\n",
        "    'GE_b': 0.0005,\n",
        "    'GE_g': 0.0015,\n",
        "    'GE_p_b': 0.5,\n",
        "    'GE_p_g': 0.02,\n",
        "    'T': 3,\n",
        "    'alphabet_size': 2,\n",
        "    'batch_size': 100,\n",
        "    'batch_size_eval': 5000,\n",
        "    'batches': 15,\n",
        "    'bptt': 6,\n",
        "    'channel_name': 'trapdoor',\n",
        "    'clip_grad_norm': 1,\n",
        "    'clip_grad_norm_enc': 0.2,\n",
        "    'clip_grad_norm_q': 100,\n",
        "    'clip_likelihood_ratio': [-10.0, 10.0],\n",
        "    'compression_flag': 0,\n",
        "    'config': './configs/capacity_estimation.json',\n",
        "    'contrastive_duplicates': 5,\n",
        "    'data_name': 'encoder',\n",
        "    'decay': 0,\n",
        "    'dropout': 0,\n",
        "    'dtype': 'binary',\n",
        "    'enc_dropout': 0.0,\n",
        "    'enc_hidden': [100, 50],\n",
        "    'enc_hidden_lstm': 50,\n",
        "    'enc_last_hidden': 1,\n",
        "    'eta_post': 0.5,\n",
        "    'eval_epoch_len': 500,\n",
        "    'eval_freq': 200,\n",
        "    'exp_name': 'cap_est',\n",
        "    'feedback': 1,\n",
        "    'hidden_size': [100, 150, 10],\n",
        "    'hidden_size_compression': [100, 2, 100],\n",
        "    'long_eval_epoch_len': 100,\n",
        "    'lr': 0.00019,\n",
        "    'lr_SGD': 0.2,\n",
        "    'max_norm_xy': 5,\n",
        "    'max_norm_y': 5,\n",
        "    'model_name': 'cap_est',\n",
        "    'noise_layer_q_std': 0.05,\n",
        "    'num_epochs': 500,\n",
        "    'optimizer': 'adam',\n",
        "    'p_bec': 0.3,\n",
        "    'p_bsc': 0.5,\n",
        "    'p_ising': 0.5,\n",
        "    'p_post': 0.5,\n",
        "    'p_trapdoor': 0.5,\n",
        "    'p_z': 0.9,\n",
        "    'q_lstm_units': 25,\n",
        "    'q_train_freq': 5,\n",
        "    'quiet': False,\n",
        "    'reset_channel': 0,\n",
        "    'run_name': 'ff',\n",
        "    's_alphabet': 2,\n",
        "    'seed': 468695,\n",
        "    'tag_name': 'temp_debugging',\n",
        "    'tensor_board_dir': './results/cap_est/encoder/ff/temp_debugging/2024-06-17_14-22-31_468695',\n",
        "    'train_epoch_len': 50,\n",
        "    'trainer_name': 'cap_est',\n",
        "    'using_wandb': 0,\n",
        "    'visualize_epoch_len': 50,\n",
        "    'weight_decay': 0.0,\n",
        "    'with_p': 0,\n",
        "    'x_dim': 1,\n",
        "    'y_im': 1,\n",
        "    'output_activation': 'linear'\n",
        "}\n",
        "\n",
        "# Initialize the data generator\n",
        "ising_data = Ising_Data(config)\n",
        "x, y = ising_data.gen_data()\n",
        "\n",
        "print(\"x data check:\", np.isnan(x).sum(), np.isinf(x).sum())\n",
        "print(\"y data check:\", np.isnan(y).sum(), np.isinf(y).sum())\n",
        "\n",
        "x_y_combined = tf.concat([x, y], axis=-1)\n",
        "\n",
        "input_shape = (config['bptt'], 2)\n",
        "complex_model_v3 = complex_lstm_model_v3(input_shape, config)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=config['lr'], clipnorm=1.0)\n",
        "complex_model_v3.compile(optimizer=optimizer, loss=tf.keras.losses.KLDivergence())\n",
        "\n",
        "def lr_scheduler(epoch, lr):\n",
        "    decay_rate = 0.9\n",
        "    decay_step = 10\n",
        "    if epoch % decay_step == 0 and epoch:\n",
        "        return lr * decay_rate\n",
        "    return lr\n",
        "\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "loss_history = LossHistory()\n",
        "\n",
        "# Train the model to capture the loss history\n",
        "complex_model_v3.fit(x_y_combined, y, epochs=config['num_epochs'], batch_size=config['batch_size'], callbacks=[callback, loss_history])\n",
        "\n",
        "# Initialize metrics\n",
        "writer = tf.summary.create_file_writer(config['tensor_board_dir'])\n",
        "metrics = ModelWithEncMetrics(writer, metric_name='training_metrics')\n",
        "\n",
        "# Initialize the capacity estimation object\n",
        "capacity_estimator = CapEstDI(complex_model_v3, ising_data, config)\n",
        "\n",
        "# Training loop with learning rate scheduler\n",
        "# Training loop with learning rate scheduler\n",
        "for epoch in range(config['num_epochs']):\n",
        "    capacity_estimator.train_epoch(epoch)\n",
        "    metrics.update_state([\n",
        "        [tf.convert_to_tensor(loss_history.losses[-1], dtype=tf.float32),\n",
        "         tf.convert_to_tensor(loss_history.losses[-1], dtype=tf.float32),\n",
        "         tf.zeros_like(tf.convert_to_tensor(loss_history.losses[-1], dtype=tf.float32)),\n",
        "         tf.zeros_like(tf.convert_to_tensor(loss_history.losses[-1], dtype=tf.float32))]\n",
        "    ])\n",
        "    metrics.log_metrics(epoch, model_name='Training')\n",
        "\n",
        "    if epoch % config['eval_freq'] == 0:\n",
        "        capacity_estimator.evaluate(epoch)\n",
        "\n",
        "\n",
        "# Monte Carlo Evaluation\n",
        "mean_loss, std_loss = capacity_estimator.mc_evaluation(num_simulations=1000)\n",
        "print(f\"Monte Carlo Evaluation - Mean Loss: {mean_loss}, Std Loss: {std_loss}\")\n",
        "\n",
        "# MDP Evaluation\n",
        "def random_policy(state):\n",
        "    return tf.random.uniform(shape=state.shape, minval=0, maxval=1)\n",
        "\n",
        "mdp_reward = capacity_estimator.mdp_evaluation(policy=random_policy, num_steps=100)\n",
        "print(f\"MDP Evaluation - Average Reward: {mdp_reward}\")\n",
        "\n",
        "# Final evaluation and reporting\n",
        "def final_evaluation(capacity_estimator):\n",
        "    # Monte Carlo Evaluation\n",
        "    mean_loss, std_loss = capacity_estimator.mc_evaluation(num_simulations=1000)\n",
        "    print(f\"Monte Carlo Evaluation - Mean Loss: {mean_loss}, Std Loss: {std_loss}\")\n",
        "\n",
        "    # MDP Evaluation\n",
        "    mdp_reward = capacity_estimator.mdp_evaluation(policy=random_policy, num_steps=100)\n",
        "    print(f\"MDP Evaluation - Average Reward: {mdp_reward}\")\n",
        "\n",
        "    return {\n",
        "        \"MC Mean Loss\": mean_loss,\n",
        "        \"MC Std Loss\": std_loss,\n",
        "        \"MDP Average Reward\": mdp_reward\n",
        "    }\n",
        "\n",
        "evaluation_results = final_evaluation(capacity_estimator)\n",
        "\n",
        "# Plotting loss and learning rate\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss', color=color)\n",
        "ax1.plot(range(len(loss_history.losses)), loss_history.losses, color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('Learning Rate', color=color)\n",
        "ax2.plot(range(len(loss_history.lr)), loss_history.lr, color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.title('Training Loss and Learning Rate')\n",
        "plt.show()\n",
        "\n",
        "# Plotting the results for Capacity Estimate, DINE Estimate, and Information Rate\n",
        "epochs = list(range(config['num_epochs']))\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Plot Capacity Estimate\n",
        "axs[0].plot(epochs, capacity_estimator.capacity_estimates, label='Capacity Estimate')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Capacity Estimate')\n",
        "axs[0].legend()\n",
        "axs[0].grid(True)\n",
        "\n",
        "# Plot DINE Estimate\n",
        "axs[1].plot(epochs, capacity_estimator.dine_estimates, label='DINE Estimate', color='orange')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('DINE Estimate')\n",
        "axs[1].legend()\n",
        "axs[1].grid(True)\n",
        "\n",
        "# Plot Information Rate\n",
        "axs[2].plot(epochs, capacity_estimator.info_rates, label='Information Rate', color='green')\n",
        "axs[2].set_xlabel('Epoch')\n",
        "axs[2].set_ylabel('Information Rate')\n",
        "axs[2].legend()\n",
        "axs[2].grid(True)\n",
        "\n",
        "plt.suptitle('Capacity Estimate, DINE Estimate, and Information Rate over Epochs')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import logging\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logger = logging.getLogger(\"logger\")\n",
        "\n",
        "class ModelMetrics(tf.keras.metrics.Metric):\n",
        "    def __init__(self, writer, metric_name='', **kwargs):\n",
        "        super(ModelMetrics, self).__init__(name=metric_name, **kwargs)\n",
        "        self.writer = writer\n",
        "        self.metric_pool = [\n",
        "            DV(metric_name='dv_xy_{}'.format(metric_name)),\n",
        "            DV(metric_name='dv_y_{}'.format(metric_name)),\n",
        "            DI(metric_name='di_{}'.format(metric_name)),\n",
        "            PMF(metric_name='p_mean'),\n",
        "            DI_bits(metric_name='di_bits')\n",
        "        ]\n",
        "\n",
        "    def update_state(self, t_y, t_xy, **kwargs):\n",
        "        t_y = [tf.convert_to_tensor(t, dtype=tf.float32) for t in t_y]\n",
        "        t_xy = [tf.convert_to_tensor(t, dtype=tf.float32) for t in t_xy]\n",
        "\n",
        "        if not isinstance(t_y, (list, tuple)) or len(t_y) < 2:\n",
        "            raise ValueError(\"Invalid t_y format. Expected lists with at least two elements.\")\n",
        "        if not isinstance(t_xy, (list, tuple)) or len(t_xy) < 2:\n",
        "            raise ValueError(\"Invalid t_xy format. Expected lists with at least two elements.\")\n",
        "\n",
        "        self.metric_pool[0].update_state(t_xy[0], t_xy[1])\n",
        "        self.metric_pool[1].update_state(t_y[0], t_y[1])\n",
        "        self.metric_pool[2].update_state(t_y[0], t_y[1], t_xy[0], t_xy[1])\n",
        "        self.metric_pool[3].update_state(t_y[0], t_y[1], t_xy[0], t_xy[1])\n",
        "\n",
        "    def result(self):\n",
        "        return [metric.result() for metric in self.metric_pool]\n",
        "\n",
        "    def reset_states(self):\n",
        "        for metric in self.metric_pool:\n",
        "            metric.reset_states()\n",
        "        return\n",
        "\n",
        "    def log_metrics(self, epoch, model_name):\n",
        "        with self.writer.as_default():\n",
        "            for metric in self.metric_pool:\n",
        "                tf.summary.scalar(metric.name, metric.result(), epoch)\n",
        "\n",
        "        msg = [\"{} Epoch: {:05d}\\t\".format(self.name, epoch)]\n",
        "        for metric in self.metric_pool:\n",
        "            result = metric.result()\n",
        "            if np.isnan(result):\n",
        "                print(f\"NaN detected in metric {metric.name} during epoch {epoch}\")\n",
        "                continue\n",
        "            msg.append(\"{:s} {:3.6f}\\t\".format(metric.name, float(result)))\n",
        "        msg.append(model_name)\n",
        "        logger.info(\"\\t\".join(msg))\n",
        "\n",
        "class DV(tf.keras.metrics.Metric):  # estimated DV loss calculation metric class\n",
        "    def __init__(self, metric_name='dv_loss', **kwargs):\n",
        "        super(DV, self).__init__(name=metric_name, **kwargs)\n",
        "        self.T = self.add_weight(name='t', initializer='zeros')\n",
        "        self.exp_T_bar = self.add_weight(name='exp_t_bar', initializer='zeros')\n",
        "        self.global_counter = self.add_weight(name='n', initializer='zeros')\n",
        "        self.global_counter_ref = self.add_weight(name='n_ref', initializer='zeros')\n",
        "\n",
        "    def update_state(self, T, T_bar, **kwargs):\n",
        "        self.T.assign_add(tf.reduce_sum(T))\n",
        "        self.exp_T_bar.assign_add(tf.reduce_sum(T_bar))\n",
        "        self.global_counter.assign_add(tf.cast(tf.reduce_prod(T.shape[:-1]), dtype=tf.float32))\n",
        "        self.global_counter_ref.assign_add(tf.cast(tf.reduce_prod(T_bar.shape[:-1]), dtype=tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        loss = self.T / (self.global_counter + 1e-10) - K.log(self.exp_T_bar / (self.global_counter_ref + 1e-10))\n",
        "        return loss\n",
        "\n",
        "class DI(tf.keras.metrics.Metric):  # estimated DI calculation metric class\n",
        "    def __init__(self, metric_name='di_loss', **kwargs):\n",
        "        super(DI, self).__init__(name=metric_name, **kwargs)\n",
        "        self.c_T = self.add_weight(name='c_t', initializer='zeros')\n",
        "        self.c_exp_T_bar = self.add_weight(name='c_exp_t_bar', initializer='zeros')\n",
        "        self.xc_T = self.add_weight(name='xc_t', initializer='zeros')\n",
        "        self.xc_exp_T_bar = self.add_weight(name='xc_exp_t_bar', initializer='zeros')\n",
        "        self.global_counter = self.add_weight(name='n', initializer='zeros')\n",
        "        self.global_counter_ref = self.add_weight(name='n_ref', initializer='zeros')\n",
        "\n",
        "    def update_state(self, c_T, c_T_bar, xc_T, xc_T_bar, **kwargs):\n",
        "        self.c_T.assign_add(tf.reduce_sum(c_T))\n",
        "        self.c_exp_T_bar.assign_add(tf.reduce_sum(c_T_bar))\n",
        "\n",
        "        self.xc_T.assign_add(tf.reduce_sum(xc_T))\n",
        "        self.xc_exp_T_bar.assign_add(tf.reduce_sum(xc_T_bar))\n",
        "\n",
        "        self.global_counter.assign_add(tf.cast(tf.reduce_prod(c_T.shape[:-1]), dtype=tf.float32))\n",
        "        self.global_counter_ref.assign_add(tf.cast(tf.reduce_prod(c_T_bar.shape[:-1]), dtype=tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        loss_y = self.c_T / (self.global_counter + 1e-10) - K.log(self.c_exp_T_bar / (self.global_counter_ref + 1e-10))\n",
        "        loss_xy = self.xc_T / (self.global_counter + 1e-10) - K.log(self.xc_exp_T_bar / (self.global_counter_ref + 1e-10))\n",
        "        return loss_xy - loss_y\n",
        "\n",
        "class DI_bits(tf.keras.metrics.Metric):  # estimated DI calculation metric class in bits\n",
        "    def __init__(self, metric_name='di_bits_loss', **kwargs):\n",
        "        super(DI_bits, self).__init__(name=metric_name, **kwargs)\n",
        "        self.c_T = self.add_weight(name='c_t', initializer='zeros')\n",
        "        self.c_exp_T_bar = self.add_weight(name='c_exp_t_bar', initializer='zeros')\n",
        "        self.xc_T = self.add_weight(name='xc_t', initializer='zeros')\n",
        "        self.xc_exp_T_bar = self.add_weight(name='xc_exp_t_bar', initializer='zeros')\n",
        "        self.global_counter = self.add_weight(name='n', initializer='zeros')\n",
        "        self.global_counter_ref = self.add_weight(name='n_ref', initializer='zeros')\n",
        "\n",
        "    def update_state(self, c_T, c_T_bar, xc_T, xc_T_bar, **kwargs):\n",
        "        self.c_T.assign_add(tf.reduce_sum(c_T))\n",
        "        self.c_exp_T_bar.assign_add(tf.reduce_sum(c_T_bar))\n",
        "\n",
        "        self.xc_T.assign_add(tf.reduce_sum(xc_T))\n",
        "        self.xc_exp_T_bar.assign_add(tf.reduce_sum(xc_T_bar))\n",
        "\n",
        "        self.global_counter.assign_add(tf.cast(tf.reduce_prod(c_T.shape[:-1]), dtype=tf.float32))\n",
        "        self.global_counter_ref.assign_add(tf.cast(tf.reduce_prod(c_T_bar.shape[:-1]), dtype=tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        loss_y = self.c_T / (self.global_counter + 1e-10) - K.log(self.c_exp_T_bar / (self.global_counter_ref + 1e-10))\n",
        "        loss_xy = self.xc_T / (self.global_counter + 1e-10) - K.log(self.xc_exp_T_bar / (self.global_counter_ref + 1e-10))\n",
        "        return (loss_xy - loss_y) / math.log(2)\n",
        "\n",
        "class PMF(tf.keras.metrics.Metric):  # estimated DV loss calculation metric class\n",
        "    def __init__(self, metric_name='p_mean', **kwargs):\n",
        "        super(PMF, self).__init__(name=metric_name, **kwargs)\n",
        "        self.p = self.add_weight(name='p', initializer='zeros')\n",
        "        self.global_counter = self.add_weight(name='n', initializer='zeros')\n",
        "\n",
        "    def update_state(self, p, **kwargs):\n",
        "        self.p.assign_add(tf.reduce_sum(p))\n",
        "        self.global_counter.assign_add(tf.cast(tf.reduce_prod(p.shape[:-1]), dtype=tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        result = self.p / (self.global_counter + 1e-10)\n",
        "        return result\n",
        "\n",
        "# class ModelWithEncMetrics(ModelMetrics):\n",
        "#     def __init__(self, writer, metric_name='ModelWithEncMetrics'):\n",
        "#         self.writer = writer\n",
        "#         self.metric_name = metric_name\n",
        "#         self.metric_pool = [\n",
        "#             tf.keras.metrics.Mean(name='capacity_estimate'),\n",
        "#             tf.keras.metrics.Mean(name='info_rate'),\n",
        "#             CustomMetric(name='di_bits_cap_est_metrics')\n",
        "#         ]\n",
        "\n",
        "#     def update_state(self, values):\n",
        "#         for metric, value in zip(self.metric_pool, values):\n",
        "#             if tf.reduce_any(tf.math.is_nan(value)):\n",
        "#                 print(f\"NaN detected in value for metric {metric.name}\")\n",
        "#                 continue\n",
        "#             metric.update_state(value)\n",
        "\n",
        "#     def result(self):\n",
        "#         return [metric.result() for metric in self.metric_pool]\n",
        "\n",
        "#     def reset_states(self):\n",
        "#         for metric in self.metric_pool:\n",
        "#             metric.reset_states()\n",
        "\n",
        "#     def log_metrics(self, epoch, model_name):\n",
        "#         with self.writer.as_default():\n",
        "#             for metric in self.metric_pool:\n",
        "#                 result = metric.result()\n",
        "#                 if tf.math.is_nan(result):\n",
        "#                     print(f\"NaN detected in metric {metric.name} during epoch {epoch}\")\n",
        "#                     continue\n",
        "#                 tf.summary.scalar(metric.name, result, epoch)\n",
        "#         msg = [\"{} Epoch: {:05d}\\t\".format(self.metric_name, epoch)]\n",
        "#         for metric in self.metric_pool:\n",
        "#             result = metric.result()\n",
        "#             if tf.math.is_nan(result):\n",
        "#                 print(f\"NaN detected in metric {metric.name} during epoch {epoch}\")\n",
        "#                 continue\n",
        "#             msg.append(\"{:s} {:3.6f}\\t\".format(metric.name, float(result)))\n",
        "#         msg.append(model_name)\n",
        "#         logger.info(\"\\t\".join(msg))\n",
        "\n",
        "# class CustomMetric(tf.keras.metrics.Metric):\n",
        "#     def __init__(self, metric_name='custom_metric', **kwargs):\n",
        "#         # Remove name from kwargs if it exists\n",
        "#         if 'name' in kwargs:\n",
        "#             del kwargs['name']\n",
        "#         super(CustomMetric, self).__init__(name=metric_name, **kwargs)\n",
        "#         self.total = self.add_weight(name='total', initializer='zeros')\n",
        "#         self.count = self.add_weight(name='count', initializer='zeros')\n",
        "\n",
        "#     def update_state(self, c_T, c_T_bar, xc_T, xc_T_bar):\n",
        "#         # Ensure that NaNs are not introduced in the calculation\n",
        "#         if tf.reduce_any(tf.math.is_nan(c_T)) or tf.reduce_any(tf.math.is_nan(c_T_bar)) or \\\n",
        "#            tf.reduce_any(tf.math.is_nan(xc_T)) or tf.reduce_any(tf.math.is_nan(xc_T_bar)):\n",
        "#             print(\"NaN detected in inputs of custom metric update_state\")\n",
        "#             return\n",
        "\n",
        "#         # Custom logic for updating state\n",
        "#         values = (c_T + c_T_bar + xc_T + xc_T_bar) / 4\n",
        "#         self.total.assign_add(tf.reduce_sum(values))\n",
        "#         self.count.assign_add(tf.cast(tf.size(values), tf.float32))\n",
        "\n",
        "#     def result(self):\n",
        "#         return tf.math.divide_no_nan(self.total, self.count)\n",
        "\n",
        "#     def reset_states(self):\n",
        "#         self.total.assign(0.0)\n",
        "#         self.count.assign(0.0)\n",
        "\n",
        "class ModelWithEncMetrics(tf.keras.metrics.Metric):\n",
        "    def __init__(self, writer, metric_name='ModelWithEncMetrics', **kwargs):\n",
        "        super(ModelWithEncMetrics, self).__init__(name=metric_name, **kwargs)\n",
        "        self.writer = writer\n",
        "        self.metric_pool = [\n",
        "            tf.keras.metrics.Mean(name='capacity_estimate'),\n",
        "            tf.keras.metrics.Mean(name='info_rate'),\n",
        "            CustomMetric(name='di_bits_cap_est_metrics')\n",
        "        ]\n",
        "\n",
        "    def update_state(self, values, **kwargs):\n",
        "        # Ensure the correct number of arguments are passed for each metric\n",
        "        for metric, value in zip(self.metric_pool, values):\n",
        "            if isinstance(metric, CustomMetric):\n",
        "                if len(value) != 4:\n",
        "                    print(f\"Skipping update for {metric.name}, expected 4 values but got {len(value)}\")\n",
        "                    continue\n",
        "                metric.update_state(*value)\n",
        "            else:\n",
        "                metric.update_state(value)\n",
        "\n",
        "    def result(self):\n",
        "        return [metric.result() for metric in self.metric_pool]\n",
        "\n",
        "    def reset_states(self):\n",
        "        for metric in self.metric_pool:\n",
        "            metric.reset_states()\n",
        "\n",
        "    def log_metrics(self, epoch, model_name):\n",
        "        with self.writer.as_default():\n",
        "            for metric in self.metric_pool:\n",
        "                result = metric.result()\n",
        "                if tf.math.is_nan(result):\n",
        "                    print(f\"NaN detected in metric {metric.name} during epoch {epoch}\")\n",
        "                    continue\n",
        "                tf.summary.scalar(metric.name, result, epoch)\n",
        "        msg = [\"{} Epoch: {:05d}\\t\".format(self.name, epoch)]\n",
        "        for metric in self.metric_pool:\n",
        "            result = metric.result()\n",
        "            if tf.math.is_nan(result):\n",
        "                print(f\"NaN detected in metric {metric.name} during epoch {epoch}\")\n",
        "                continue\n",
        "            msg.append(\"{:s} {:3.6f}\\t\".format(metric.name, float(result)))\n",
        "        msg.append(model_name)\n",
        "        logger.info(\"\\t\".join(msg))\n",
        "\n",
        "\n",
        "class CustomMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='custom_metric', **kwargs):\n",
        "        # Remove name from kwargs if it exists\n",
        "        if 'name' in kwargs:\n",
        "            del kwargs['name']\n",
        "        super(CustomMetric, self).__init__(name=name, **kwargs)\n",
        "        self.total = self.add_weight(name='total', initializer='zeros')\n",
        "        self.count = self.add_weight(name='count', initializer='zeros')\n",
        "\n",
        "    def update_state(self, c_T, c_T_bar, xc_T, xc_T_bar, **kwargs):\n",
        "        # Ensure that NaNs are not introduced in the calculation\n",
        "        if tf.reduce_any(tf.math.is_nan(c_T)) or tf.reduce_any(tf.math.is_nan(c_T_bar)) or \\\n",
        "           tf.reduce_any(tf.math.is_nan(xc_T)) or tf.reduce_any(tf.math.is_nan(xc_T_bar)):\n",
        "            print(\"NaN detected in inputs of custom metric update_state\")\n",
        "            return tf.constant(0.0)  # Ensure a return value even in the case of NaN detection\n",
        "\n",
        "        # Custom logic for updating state\n",
        "        values = (c_T + c_T_bar + xc_T + xc_T_bar) / 4\n",
        "        self.total.assign_add(tf.reduce_sum(values))\n",
        "        self.count.assign_add(tf.cast(tf.size(values), tf.float32))\n",
        "        return tf.constant(1.0)  # Ensure a return value\n",
        "\n",
        "    def result(self):\n",
        "        return tf.math.divide_no_nan(self.total, self.count)\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.total.assign(0.0)\n",
        "        self.count.assign(0.0)\n",
        "\n",
        "\n",
        "\n",
        "class CapEstDI(object):\n",
        "    def __init__(self, model, data, config):\n",
        "        self.model = model\n",
        "        self.data_iterators = data\n",
        "        self.config = config\n",
        "        self.loss_fn = tf.keras.losses.Huber()\n",
        "        self.learning_rate = config['lr']\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "        self.metrics = ModelWithEncMetrics(tf.summary.create_file_writer(config['tensor_board_dir']), metric_name='cap_est_metrics')\n",
        "        self.feedback = (config['feedback'] == 1)\n",
        "        self.T = config['T']\n",
        "        self.capacity_estimates = []\n",
        "        self.dine_estimates = []\n",
        "        self.info_rates = []\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        self.metrics.reset_states()\n",
        "        self.reset_model_states()\n",
        "\n",
        "        for _ in range(self.config['batches']):\n",
        "            x, y = self.data_iterators.gen_data()\n",
        "            x_y_combined = tf.concat([x, y], axis=-1)\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = self.model(x_y_combined, training=True)\n",
        "                loss = self.loss_fn(y, predictions)\n",
        "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "            c_T = predictions\n",
        "            c_T_bar = tf.math.exp(predictions)\n",
        "            xc_T = predictions * y\n",
        "            xc_T_bar = tf.math.exp(predictions * y)\n",
        "\n",
        "            # Debug print statements to check intermediate values\n",
        "            # print(f\"c_T: {c_T.numpy()[:5]}, c_T_bar: {c_T_bar.numpy()[:5]}\")\n",
        "            # print(f\"xc_T: {xc_T.numpy()[:5]}, xc_T_bar: {xc_T_bar.numpy()[:5]}\")\n",
        "\n",
        "        self.metrics.metric_pool[2].update_state(c_T, c_T_bar, xc_T, xc_T_bar)\n",
        "\n",
        "        self.metrics.log_metrics(epoch, model_name=\"Training\")\n",
        "\n",
        "        # Store estimates\n",
        "        self.capacity_estimates.append(self.metrics.metric_pool[0].result().numpy())\n",
        "        self.dine_estimates.append(self.metrics.metric_pool[2].result().numpy())\n",
        "        self.info_rates.append(self.metrics.metric_pool[1].result().numpy())\n",
        "\n",
        "    def evaluate(self, epoch):\n",
        "        self.metrics.reset_states()\n",
        "        x, y = self.data_iterators.gen_data()\n",
        "        x_y_combined = tf.concat([x, y], axis=-1)\n",
        "        predictions = self.model(x_y_combined, training=False)\n",
        "        loss = self.loss_fn(y, predictions)\n",
        "        self.metrics.update_state([loss, loss, tf.random.uniform(shape=[self.config['batch_size'], 1], minval=0, maxval=1)])\n",
        "        self.metrics.log_metrics(epoch, model_name=\"Evaluation\")\n",
        "\n",
        "    def train(self, num_epochs=None):\n",
        "        if num_epochs is None:\n",
        "            num_epochs = self.config.get('num_epochs', 500)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            if epoch % self.config['eval_freq'] == 0:\n",
        "                self.evaluate(epoch)\n",
        "            self.train_epoch(epoch)\n",
        "\n",
        "        self.evaluate(num_epochs)\n",
        "\n",
        "    def reset_model_states(self):\n",
        "        for layer in self.model.layers:\n",
        "            if hasattr(layer, 'reset_states') and layer.stateful:\n",
        "                layer.reset_states()\n",
        "\n",
        "    def mc_evaluation(self, num_simulations=1000):\n",
        "        results = []\n",
        "        for _ in range(num_simulations):\n",
        "            x, y = self.data_iterators.gen_data()\n",
        "            x_y_combined = tf.concat([x, y], axis=-1)\n",
        "            predictions = self.model(x_y_combined, training=False)\n",
        "            loss = self.loss_fn(y, predictions)\n",
        "            results.append(loss.numpy())\n",
        "        return np.mean(results), np.std(results)\n",
        "\n",
        "    def mdp_evaluation(self, policy, num_steps=100):\n",
        "        state = self.data_iterators.gen_data()[0]\n",
        "        total_reward = 0\n",
        "        for _ in range(num_steps):\n",
        "            action = policy(state)\n",
        "            next_state, reward = self.data_iterators.gen_data()\n",
        "            total_reward += reward.numpy()\n",
        "            state = next_state\n",
        "        return total_reward / num_steps\n",
        "\n",
        "class Ising_Data(object):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.p_x = 0.4503\n",
        "        self.p_ch = 0.5\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.bptt = config['bptt']\n",
        "        self.ising_ch_logits = self.gen_logits(self.p_ch)\n",
        "        self.ising_x_logits = self.gen_logits(1 - self.p_x)\n",
        "        self.initialize_channel()\n",
        "\n",
        "    def gen_logits(self, p):\n",
        "        p_t = p * tf.ones(shape=[self.batch_size, 1], dtype=tf.float32)\n",
        "        p_bar_t = tf.ones_like(p_t) - p_t\n",
        "        logits = tf.math.log(tf.concat([p_bar_t, p_t], axis=1) + 1e-10)\n",
        "        return logits\n",
        "\n",
        "    def gen_data(self):\n",
        "        y_l = []\n",
        "        x_l = []\n",
        "        for t in range(self.bptt):\n",
        "            self.encoder()\n",
        "            x_l.append(self.x)\n",
        "            self.channel()\n",
        "            y_l.append(self.y)\n",
        "        x = tf.concat(x_l, axis=1)\n",
        "        y = tf.concat(y_l, axis=1)\n",
        "        return x, y\n",
        "\n",
        "    def initialize_channel(self):\n",
        "        for step in [0, 1]:\n",
        "            self.encoder(step)\n",
        "            self.channel(step)\n",
        "\n",
        "    def encoder(self, step=None):\n",
        "        if step == 0:\n",
        "            self.s_past = tf.zeros(shape=[self.batch_size, 1, 1], dtype=tf.float32)\n",
        "            self.x = tf.cast(tf.random.categorical(logits=self.ising_x_logits, num_samples=1), dtype=tf.float32)\n",
        "            self.x = tf.expand_dims(self.x, axis=-1)\n",
        "        elif step == 1:\n",
        "            self.x = self.x\n",
        "        else:\n",
        "            z = tf.cast(tf.random.categorical(logits=self.ising_x_logits, num_samples=1), dtype=tf.float32)\n",
        "            z = tf.expand_dims(z, axis=-1)\n",
        "            x_p = tf.math.floormod(self.x + z, 2)\n",
        "            x_new = tf.where(tf.equal(self.y, self.s_past), self.s, x_p)\n",
        "            self.x = x_new\n",
        "\n",
        "    def channel(self, step=None):\n",
        "        z = tf.cast(tf.random.categorical(logits=self.ising_ch_logits, num_samples=1), dtype=tf.float32)\n",
        "        z = tf.expand_dims(z, axis=-1)\n",
        "        if step == 0:\n",
        "            self.s = self.s_past\n",
        "        self.y = tf.where(tf.equal(z, 0), self.x, self.s)\n",
        "        self.s_past = self.s\n",
        "        self.s = self.x\n",
        "\n",
        "def complex_lstm_model_v3(input_shape, config):\n",
        "    randN_05 = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
        "    bias_init = tf.keras.initializers.Constant(0.01)\n",
        "\n",
        "    if config['compression_flag'] == 1:\n",
        "        DV_hidden = config['hidden_size_compression']\n",
        "    else:\n",
        "        DV_hidden = config['hidden_size']\n",
        "\n",
        "    max_norm = config['max_norm_y']\n",
        "\n",
        "    inputs = tf.keras.Input(shape=input_shape, batch_size=config['batch_size'])\n",
        "    x = tf.keras.layers.LSTM(DV_hidden[0], return_sequences=True, stateful=True, dropout=config['dropout'], recurrent_dropout=config['dropout'])(inputs)\n",
        "    x = tf.keras.layers.Dense(DV_hidden[1], activation=\"relu\", kernel_initializer=randN_05, bias_initializer=bias_init)(x)\n",
        "    x = tf.keras.layers.Dense(DV_hidden[2], activation=\"relu\", kernel_initializer=randN_05, bias_initializer=bias_init)(x)\n",
        "    outputs = tf.keras.layers.Dense(1, activation=config['output_activation'], kernel_initializer=randN_05, bias_initializer=bias_init, kernel_constraint=tf.keras.constraints.MaxNorm(max_value=max_norm))(x)\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "class LossHistory(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.lr = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.lr.append(self.model.optimizer.learning_rate.numpy())\n",
        "\n",
        "def negative_log_likelihood(y_true, y_pred):\n",
        "    return -tf.reduce_mean(tf.reduce_sum(y_true * tf.math.log(y_pred + 1e-10), axis=-1))\n",
        "\n",
        "# Configuration\n",
        "config = {\n",
        "    'GE_b': 0.0005,\n",
        "    'GE_g': 0.0015,\n",
        "    'GE_p_b': 0.5,\n",
        "    'GE_p_g': 0.02,\n",
        "    'T': 3,\n",
        "    'alphabet_size': 2,\n",
        "    'batch_size': 100,\n",
        "    'batch_size_eval': 5000,\n",
        "    'batches': 15,\n",
        "    'bptt': 6,\n",
        "    'channel_name': 'trapdoor',\n",
        "    'clip_grad_norm': 1,\n",
        "    'clip_grad_norm_enc': 0.2,\n",
        "    'clip_grad_norm_q': 100,\n",
        "    'clip_likelihood_ratio': [-10.0, 10.0],\n",
        "    'compression_flag': 0,\n",
        "    'config': './configs/capacity_estimation.json',\n",
        "    'contrastive_duplicates': 5,\n",
        "    'data_name': 'encoder',\n",
        "    'decay': 0,\n",
        "    'dropout': 0,\n",
        "    'dtype': 'binary',\n",
        "    'enc_dropout': 0.0,\n",
        "    'enc_hidden': [100, 50],\n",
        "    'enc_hidden_lstm': 50,\n",
        "    'enc_last_hidden': 1,\n",
        "    'eta_post': 0.5,\n",
        "    'eval_epoch_len': 500,\n",
        "    'eval_freq': 200,\n",
        "    'exp_name': 'cap_est',\n",
        "    'feedback': 1,\n",
        "    'hidden_size': [100, 150, 10],\n",
        "    'hidden_size_compression': [100, 2, 100],\n",
        "    'long_eval_epoch_len': 100,\n",
        "    'lr': 0.00019,\n",
        "    'lr_SGD': 0.2,\n",
        "    'max_norm_xy': 5,\n",
        "    'max_norm_y': 5,\n",
        "    'model_name': 'cap_est',\n",
        "    'noise_layer_q_std': 0.05,\n",
        "    'num_epochs': 500,\n",
        "    'optimizer': 'adam',\n",
        "    'p_bec': 0.3,\n",
        "    'p_bsc': 0.5,\n",
        "    'p_ising': 0.5,\n",
        "    'p_post': 0.5,\n",
        "    'p_trapdoor': 0.5,\n",
        "    'p_z': 0.9,\n",
        "    'q_lstm_units': 25,\n",
        "    'q_train_freq': 5,\n",
        "    'quiet': False,\n",
        "    'reset_channel': 0,\n",
        "    'run_name': 'ff',\n",
        "    's_alphabet': 2,\n",
        "    'seed': 468695,\n",
        "    'tag_name': 'temp_debugging',\n",
        "    'tensor_board_dir': './results/cap_est/encoder/ff/temp_debugging/2024-06-17_14-22-31_468695',\n",
        "    'train_epoch_len': 50,\n",
        "    'trainer_name': 'cap_est',\n",
        "    'using_wandb': 0,\n",
        "    'visualize_epoch_len': 50,\n",
        "    'weight_decay': 0.0,\n",
        "    'with_p': 0,\n",
        "    'x_dim': 1,\n",
        "    'y_im': 1,\n",
        "    'output_activation': 'linear'\n",
        "}\n",
        "\n",
        "# Initialize the data generator\n",
        "ising_data = Ising_Data(config)\n",
        "x, y = ising_data.gen_data()\n",
        "\n",
        "print(\"x data check:\", np.isnan(x).sum(), np.isinf(x).sum())\n",
        "print(\"y data check:\", np.isnan(y).sum(), np.isinf(y).sum())\n",
        "\n",
        "x_y_combined = tf.concat([x, y], axis=-1)\n",
        "\n",
        "input_shape = (config['bptt'], 2)\n",
        "complex_model_v3 = complex_lstm_model_v3(input_shape, config)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=config['lr'], clipnorm=1.0)\n",
        "complex_model_v3.compile(optimizer=optimizer, loss=negative_log_likelihood)\n",
        "\n",
        "def lr_scheduler(epoch, lr):\n",
        "    decay_rate = 0.9\n",
        "    decay_step = 10\n",
        "    if epoch % decay_step == 0 and epoch:\n",
        "        return lr * decay_rate\n",
        "    return lr\n",
        "\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "loss_history = LossHistory()\n",
        "\n",
        "# Train the model to capture the loss history\n",
        "complex_model_v3.fit(x_y_combined, y, epochs=config['num_epochs'], batch_size=config['batch_size'], callbacks=[callback, loss_history])\n",
        "\n",
        "# Initialize metrics\n",
        "writer = tf.summary.create_file_writer(config['tensor_board_dir'])\n",
        "metrics = ModelWithEncMetrics(writer, metric_name='training_metrics')\n",
        "\n",
        "# Initialize the capacity estimation object\n",
        "capacity_estimator = CapEstDI(complex_model_v3, ising_data, config)\n",
        "\n",
        "# Training loop with learning rate scheduler\n",
        "# Training loop with learning rate scheduler\n",
        "for epoch in range(config['num_epochs']):\n",
        "    capacity_estimator.train_epoch(epoch)\n",
        "    metrics.update_state([\n",
        "        [tf.convert_to_tensor(loss_history.losses[-1], dtype=tf.float32),\n",
        "         tf.convert_to_tensor(loss_history.losses[-1], dtype=tf.float32),\n",
        "         tf.zeros_like(tf.convert_to_tensor(loss_history.losses[-1], dtype=tf.float32)),\n",
        "         tf.zeros_like(tf.convert_to_tensor(loss_history.losses[-1], dtype=tf.float32))]\n",
        "    ])\n",
        "    metrics.log_metrics(epoch, model_name='Training')\n",
        "\n",
        "    if epoch % config['eval_freq'] == 0:\n",
        "        capacity_estimator.evaluate(epoch)\n",
        "\n",
        "\n",
        "# Monte Carlo Evaluation\n",
        "mean_loss, std_loss = capacity_estimator.mc_evaluation(num_simulations=1000)\n",
        "print(f\"Monte Carlo Evaluation - Mean Loss: {mean_loss}, Std Loss: {std_loss}\")\n",
        "\n",
        "# MDP Evaluation\n",
        "def random_policy(state):\n",
        "    return tf.random.uniform(shape=state.shape, minval=0, maxval=1)\n",
        "\n",
        "mdp_reward = capacity_estimator.mdp_evaluation(policy=random_policy, num_steps=100)\n",
        "print(f\"MDP Evaluation - Average Reward: {mdp_reward}\")\n",
        "\n",
        "# Final evaluation and reporting\n",
        "def final_evaluation(capacity_estimator):\n",
        "    # Monte Carlo Evaluation\n",
        "    mean_loss, std_loss = capacity_estimator.mc_evaluation(num_simulations=1000)\n",
        "    print(f\"Monte Carlo Evaluation - Mean Loss: {mean_loss}, Std Loss: {std_loss}\")\n",
        "\n",
        "    # MDP Evaluation\n",
        "    mdp_reward = capacity_estimator.mdp_evaluation(policy=random_policy, num_steps=100)\n",
        "    print(f\"MDP Evaluation - Average Reward: {mdp_reward}\")\n",
        "\n",
        "    return {\n",
        "        \"MC Mean Loss\": mean_loss,\n",
        "        \"MC Std Loss\": std_loss,\n",
        "        \"MDP Average Reward\": mdp_reward\n",
        "    }\n",
        "\n",
        "evaluation_results = final_evaluation(capacity_estimator)\n",
        "\n",
        "# Plotting loss and learning rate\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss', color=color)\n",
        "ax1.plot(range(len(loss_history.losses)), loss_history.losses, color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('Learning Rate', color=color)\n",
        "ax2.plot(range(len(loss_history.lr)), loss_history.lr, color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.title('Training Loss and Learning Rate')\n",
        "plt.show()\n",
        "\n",
        "# Plotting the results for Capacity Estimate, DINE Estimate, and Information Rate\n",
        "epochs = list(range(config['num_epochs']))\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Plot Capacity Estimate\n",
        "axs[0].plot(epochs, capacity_estimator.capacity_estimates, label='Capacity Estimate')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Capacity Estimate')\n",
        "axs[0].legend()\n",
        "axs[0].grid(True)\n",
        "\n",
        "# Plot DINE Estimate\n",
        "axs[1].plot(epochs, capacity_estimator.dine_estimates, label='DINE Estimate', color='orange')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('DINE Estimate')\n",
        "axs[1].legend()\n",
        "axs[1].grid(True)\n",
        "\n",
        "# Plot Information Rate\n",
        "axs[2].plot(epochs, capacity_estimator.info_rates, label='Information Rate', color='green')\n",
        "axs[2].set_xlabel('Epoch')\n",
        "axs[2].set_ylabel('Information Rate')\n",
        "axs[2].legend()\n",
        "axs[2].grid(True)\n",
        "\n",
        "plt.suptitle('Capacity Estimate, DINE Estimate, and Information Rate over Epochs')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Id4VOu9VyjIF",
        "outputId": "fcbc2a22-8ae4-44b6-81d3-1116783d3835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x data check: 0 0\n",
            "y data check: 0 0\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 1/500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step - loss: 2.8238 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8067 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7899 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.7734 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7571 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7408 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7248 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7083 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6924 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0001900000061141327.\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6775 - lr: 1.9000e-04\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.00017100000550271944.\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6619 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6486 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6344 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6212 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.6064 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.5905 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5757 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.5616 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5484 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.00017100000695791095.\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5305 - lr: 1.7100e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.00015390000626211986.\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5148 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 22: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4999 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 23: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4861 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 24: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4688 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 25: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4522 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 26: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4350 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 27: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4166 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 28: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4006 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 29: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3812 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 30: LearningRateScheduler setting learning rate to 0.00015390000771731138.\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3670 - lr: 1.5390e-04\n",
            "\n",
            "Epoch 31: LearningRateScheduler setting learning rate to 0.00013851000694558026.\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3415 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 32: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.3146 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 33: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.3052 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 34: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2825 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 35: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2581 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 36: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2399 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 37: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2212 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 38: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1945 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 39: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1622 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 40: LearningRateScheduler setting learning rate to 0.0001385100040351972.\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1269 - lr: 1.3851e-04\n",
            "\n",
            "Epoch 41: LearningRateScheduler setting learning rate to 0.00012465900363167748.\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0974 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 42: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.0779 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 43: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.0445 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 44: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0147 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 45: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.9852 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 46: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.9545 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 47: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.9070 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 48: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.8700 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 49: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.8315 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 50: LearningRateScheduler setting learning rate to 0.0001246590109076351.\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.7881 - lr: 1.2466e-04\n",
            "\n",
            "Epoch 51: LearningRateScheduler setting learning rate to 0.00011219310981687158.\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.7382 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 52: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.7079 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 53: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6605 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 54: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.6171 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 55: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.5817 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 56: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.5361 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 57: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.5024 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 58: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.4690 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 59: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.4364 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 60: LearningRateScheduler setting learning rate to 0.00011219311272725463.\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.4009 - lr: 1.1219e-04\n",
            "\n",
            "Epoch 61: LearningRateScheduler setting learning rate to 0.00010097380145452916.\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.3695 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 62: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.3428 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 63: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3152 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 64: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2914 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 65: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2667 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 66: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2439 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 67: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2212 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 68: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1988 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 69: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1784 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 70: LearningRateScheduler setting learning rate to 0.00010097380436491221.\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1573 - lr: 1.0097e-04\n",
            "\n",
            "Epoch 71: LearningRateScheduler setting learning rate to 9.087642392842099e-05.\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1356 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 72: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1177 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 73: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1004 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 74: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0820 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 75: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0646 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 76: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0481 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 77: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0315 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 78: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0149 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 79: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9986 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 80: LearningRateScheduler setting learning rate to 9.087642683880404e-05.\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9824 - lr: 9.0876e-05\n",
            "\n",
            "Epoch 81: LearningRateScheduler setting learning rate to 8.178878415492364e-05.\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.9662 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 82: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.9521 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 83: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9383 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 84: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9246 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 85: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.9111 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 86: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8977 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 87: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8846 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 88: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.8717 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 89: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8586 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 90: LearningRateScheduler setting learning rate to 8.17887848825194e-05.\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8457 - lr: 8.1789e-05\n",
            "\n",
            "Epoch 91: LearningRateScheduler setting learning rate to 7.360990639426745e-05.\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8333 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 92: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.8217 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 93: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8107 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 94: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7994 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 95: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7883 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 96: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7776 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 97: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7671 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 98: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7563 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 99: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7458 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 100: LearningRateScheduler setting learning rate to 7.360990275628865e-05.\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7356 - lr: 7.3610e-05\n",
            "\n",
            "Epoch 101: LearningRateScheduler setting learning rate to 6.624891248065979e-05.\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7251 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 102: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7160 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 103: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7068 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 104: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6976 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 105: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6886 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 106: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6798 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 107: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6709 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 108: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6623 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 109: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6535 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 110: LearningRateScheduler setting learning rate to 6.624891102546826e-05.\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6448 - lr: 6.6249e-05\n",
            "\n",
            "Epoch 111: LearningRateScheduler setting learning rate to 5.962401992292144e-05.\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6361 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 112: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6286 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 113: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6208 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 114: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6134 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 115: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6058 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 116: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5984 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 117: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5910 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 118: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5835 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 119: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5761 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 120: LearningRateScheduler setting learning rate to 5.9624020650517195e-05.\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5689 - lr: 5.9624e-05\n",
            "\n",
            "Epoch 121: LearningRateScheduler setting learning rate to 5.366161858546548e-05.\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5617 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 122: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5552 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 123: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5486 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 124: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5423 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 125: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5359 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 126: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5297 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 127: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5232 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 128: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5170 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 129: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5108 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 130: LearningRateScheduler setting learning rate to 5.366161713027395e-05.\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5047 - lr: 5.3662e-05\n",
            "\n",
            "Epoch 131: LearningRateScheduler setting learning rate to 4.829545541724656e-05.\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4984 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 132: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4930 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 133: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4874 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 134: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4820 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 135: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4765 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 136: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4711 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 137: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4657 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 138: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4604 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 139: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4551 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 140: LearningRateScheduler setting learning rate to 4.82954565086402e-05.\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4495 - lr: 4.8295e-05\n",
            "\n",
            "Epoch 141: LearningRateScheduler setting learning rate to 4.346591085777618e-05.\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4444 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 142: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4396 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 143: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4349 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 144: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4302 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 145: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4254 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 146: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4208 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 147: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4162 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 148: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4115 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 149: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4070 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 150: LearningRateScheduler setting learning rate to 4.34659123129677e-05.\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4023 - lr: 4.3466e-05\n",
            "\n",
            "Epoch 151: LearningRateScheduler setting learning rate to 3.911932108167093e-05.\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3978 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 152: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3936 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 153: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3894 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 154: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3854 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 155: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3813 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 156: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3772 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 157: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3732 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 158: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3692 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 159: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3652 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 160: LearningRateScheduler setting learning rate to 3.911932071787305e-05.\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3611 - lr: 3.9119e-05\n",
            "\n",
            "Epoch 161: LearningRateScheduler setting learning rate to 3.520738864608575e-05.\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3571 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 162: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3536 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 163: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3500 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 164: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3465 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 165: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3429 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 166: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3393 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 167: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3357 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 168: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3324 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 169: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3287 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 170: LearningRateScheduler setting learning rate to 3.5207387554692104e-05.\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3252 - lr: 3.5207e-05\n",
            "\n",
            "Epoch 171: LearningRateScheduler setting learning rate to 3.16866487992229e-05.\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3218 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 172: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3186 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 173: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3155 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 174: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3123 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 175: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3093 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 176: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3062 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 177: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3030 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 178: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3000 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 179: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2969 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 180: LearningRateScheduler setting learning rate to 3.1686649890616536e-05.\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2938 - lr: 3.1687e-05\n",
            "\n",
            "Epoch 181: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2907 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 182: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2879 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 183: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2852 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 184: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2825 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 185: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2797 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 186: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2770 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 187: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2742 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 188: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2716 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 189: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2689 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 190: LearningRateScheduler setting learning rate to 2.8517984901554883e-05.\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2661 - lr: 2.8518e-05\n",
            "\n",
            "Epoch 191: LearningRateScheduler setting learning rate to 2.5666186411399396e-05.\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2635 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 192: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2611 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 193: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2586 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 194: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2561 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 195: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2538 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 196: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2513 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 197: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2489 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 198: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2466 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 199: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2441 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 200: LearningRateScheduler setting learning rate to 2.5666186047601514e-05.\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2417 - lr: 2.5666e-05\n",
            "\n",
            "Epoch 201: LearningRateScheduler setting learning rate to 2.3099567442841362e-05.\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2394 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 202: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2372 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 203: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2351 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 204: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2329 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 205: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2309 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 206: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2286 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 207: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2265 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 208: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2243 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 209: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2223 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 210: LearningRateScheduler setting learning rate to 2.3099568352336064e-05.\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2201 - lr: 2.3100e-05\n",
            "\n",
            "Epoch 211: LearningRateScheduler setting learning rate to 2.078961151710246e-05.\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2180 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 212: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2162 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 213: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2143 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 214: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2124 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 215: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2105 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 216: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2085 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 217: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2066 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 218: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2048 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 219: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2029 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 220: LearningRateScheduler setting learning rate to 2.0789611880900338e-05.\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2010 - lr: 2.0790e-05\n",
            "\n",
            "Epoch 221: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1992 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 222: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1974 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 223: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1958 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 224: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1941 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 225: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1924 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 226: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1907 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 227: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1890 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 228: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1874 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 229: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1858 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 230: LearningRateScheduler setting learning rate to 1.8710650692810304e-05.\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1840 - lr: 1.8711e-05\n",
            "\n",
            "Epoch 231: LearningRateScheduler setting learning rate to 1.6839585623529273e-05.\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1824 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 232: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1808 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 233: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1794 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 234: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1779 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 235: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1764 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 236: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.1749 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 237: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1734 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 238: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1719 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 239: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1704 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 240: LearningRateScheduler setting learning rate to 1.6839585441630334e-05.\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1689 - lr: 1.6840e-05\n",
            "\n",
            "Epoch 241: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1675 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 242: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1661 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 243: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1648 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 244: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1635 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 245: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1621 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 246: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1608 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 247: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1594 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 248: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1581 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 249: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1568 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 250: LearningRateScheduler setting learning rate to 1.51556268974673e-05.\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1556 - lr: 1.5156e-05\n",
            "\n",
            "Epoch 251: LearningRateScheduler setting learning rate to 1.3640064207720571e-05.\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1542 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 252: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1530 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 253: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1518 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 254: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1506 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 255: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1495 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 256: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1483 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 257: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1471 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 258: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1459 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 259: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1447 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 260: LearningRateScheduler setting learning rate to 1.364006402582163e-05.\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1435 - lr: 1.3640e-05\n",
            "\n",
            "Epoch 261: LearningRateScheduler setting learning rate to 1.2276057623239467e-05.\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1424 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 262: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1413 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 263: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1403 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 264: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1392 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 265: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1381 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 266: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1371 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 267: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1360 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 268: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1349 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 269: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1339 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 270: LearningRateScheduler setting learning rate to 1.2276057532289997e-05.\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1329 - lr: 1.2276e-05\n",
            "\n",
            "Epoch 271: LearningRateScheduler setting learning rate to 1.1048451779060998e-05.\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1319 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 272: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1309 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 273: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1299 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 274: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1290 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 275: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1281 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 276: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1272 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 277: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1262 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 278: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1252 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 279: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1243 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 280: LearningRateScheduler setting learning rate to 1.1048451597162057e-05.\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1234 - lr: 1.1048e-05\n",
            "\n",
            "Epoch 281: LearningRateScheduler setting learning rate to 9.943606437445851e-06.\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1224 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 282: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1216 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 283: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1208 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 284: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1199 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 285: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1190 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 286: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1182 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 287: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1174 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 288: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1165 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 289: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1157 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 290: LearningRateScheduler setting learning rate to 9.943606528395321e-06.\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1148 - lr: 9.9436e-06\n",
            "\n",
            "Epoch 291: LearningRateScheduler setting learning rate to 8.94924587555579e-06.\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1140 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 292: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1133 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 293: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1125 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 294: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1118 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 295: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1110 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 296: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1103 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 297: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1095 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 298: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1088 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 299: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1080 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 300: LearningRateScheduler setting learning rate to 8.94924596650526e-06.\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1073 - lr: 8.9492e-06\n",
            "\n",
            "Epoch 301: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1065 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 302: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1058 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 303: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1052 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 304: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.1045 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 305: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1038 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 306: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1032 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 307: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1025 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 308: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1018 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 309: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.1011 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 310: LearningRateScheduler setting learning rate to 8.054321369854733e-06.\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1005 - lr: 8.0543e-06\n",
            "\n",
            "Epoch 311: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0998 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 312: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0992 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 313: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0986 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 314: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0980 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 315: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0974 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 316: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0967 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 317: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0962 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 318: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0956 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 319: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0949 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 320: LearningRateScheduler setting learning rate to 7.24888923286926e-06.\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0944 - lr: 7.2489e-06\n",
            "\n",
            "Epoch 321: LearningRateScheduler setting learning rate to 6.524000309582334e-06.\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0938 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 322: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0932 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 323: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0927 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 324: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0921 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 325: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0916 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 326: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0911 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 327: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0905 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 328: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0899 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 329: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0894 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 330: LearningRateScheduler setting learning rate to 6.524000127683394e-06.\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0889 - lr: 6.5240e-06\n",
            "\n",
            "Epoch 331: LearningRateScheduler setting learning rate to 5.871600114915055e-06.\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0884 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 332: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0879 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 333: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0874 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 334: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0869 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 335: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0864 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 336: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0859 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 337: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0854 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 338: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0850 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 339: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0845 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 340: LearningRateScheduler setting learning rate to 5.871600023965584e-06.\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0840 - lr: 5.8716e-06\n",
            "\n",
            "Epoch 341: LearningRateScheduler setting learning rate to 5.284440021569026e-06.\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0835 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 342: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0830 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 343: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0827 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 344: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0822 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 345: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0818 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 346: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0814 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 347: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0809 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 348: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0805 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 349: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0800 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 350: LearningRateScheduler setting learning rate to 5.28443979419535e-06.\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0796 - lr: 5.2844e-06\n",
            "\n",
            "Epoch 351: LearningRateScheduler setting learning rate to 4.755995814775815e-06.\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0791 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 352: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0788 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 353: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0784 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 354: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0780 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 355: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0776 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 356: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0772 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 357: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0768 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 358: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0764 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 359: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0761 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 360: LearningRateScheduler setting learning rate to 4.755995632876875e-06.\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0757 - lr: 4.7560e-06\n",
            "\n",
            "Epoch 361: LearningRateScheduler setting learning rate to 4.280396069589187e-06.\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0753 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 362: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0749 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 363: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0746 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 364: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0742 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 365: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0739 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 366: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0735 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 367: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0732 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 368: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0728 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 369: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0725 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 370: LearningRateScheduler setting learning rate to 4.280395842215512e-06.\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0721 - lr: 4.2804e-06\n",
            "\n",
            "Epoch 371: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0718 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 372: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0714 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 373: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0712 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 374: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0708 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 375: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0705 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 376: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0702 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 377: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0699 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 378: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0696 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 379: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0692 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 380: LearningRateScheduler setting learning rate to 3.852356257993961e-06.\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0689 - lr: 3.8524e-06\n",
            "\n",
            "Epoch 381: LearningRateScheduler setting learning rate to 3.467120632194565e-06.\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0686 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 382: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0684 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 383: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0680 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 384: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0678 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 385: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0675 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 386: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0672 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 387: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0669 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 388: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0667 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 389: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0663 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 390: LearningRateScheduler setting learning rate to 3.4671206776692998e-06.\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0661 - lr: 3.4671e-06\n",
            "\n",
            "Epoch 391: LearningRateScheduler setting learning rate to 3.12040860990237e-06.\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0658 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 392: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0655 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 393: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0653 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 394: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0651 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 395: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0648 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 396: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0645 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 397: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0642 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 398: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0640 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 399: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0638 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 400: LearningRateScheduler setting learning rate to 3.1204085644276347e-06.\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0635 - lr: 3.1204e-06\n",
            "\n",
            "Epoch 401: LearningRateScheduler setting learning rate to 2.8083677079848714e-06.\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0632 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 402: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 403: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0628 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 404: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0625 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 405: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0624 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 406: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0621 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 407: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0619 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 408: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0616 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 409: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0614 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 410: LearningRateScheduler setting learning rate to 2.8083677534596063e-06.\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0612 - lr: 2.8084e-06\n",
            "\n",
            "Epoch 411: LearningRateScheduler setting learning rate to 2.527530978113646e-06.\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0610 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 412: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0608 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 413: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0605 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 414: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0604 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 415: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0601 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 416: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0599 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 417: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0597 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 418: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0595 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 419: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0593 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 420: LearningRateScheduler setting learning rate to 2.5275310235883808e-06.\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0591 - lr: 2.5275e-06\n",
            "\n",
            "Epoch 421: LearningRateScheduler setting learning rate to 2.2747779212295426e-06.\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0589 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 422: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0588 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 423: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0585 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 424: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0584 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 425: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0582 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 426: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0580 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 427: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0578 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 428: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0576 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 429: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0574 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 430: LearningRateScheduler setting learning rate to 2.274777898492175e-06.\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0572 - lr: 2.2748e-06\n",
            "\n",
            "Epoch 431: LearningRateScheduler setting learning rate to 2.0473001086429576e-06.\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0571 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 432: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0569 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 433: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0568 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 434: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0566 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 435: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0564 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 436: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0562 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 437: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0561 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 438: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0559 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 439: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0558 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 440: LearningRateScheduler setting learning rate to 2.04730008590559e-06.\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0556 - lr: 2.0473e-06\n",
            "\n",
            "Epoch 441: LearningRateScheduler setting learning rate to 1.8425700773150312e-06.\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0554 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 442: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0553 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 443: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0551 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 444: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0550 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 445: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0548 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 446: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0547 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 447: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0545 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 448: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0544 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 449: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0543 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 450: LearningRateScheduler setting learning rate to 1.8425701000523986e-06.\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0541 - lr: 1.8426e-06\n",
            "\n",
            "Epoch 451: LearningRateScheduler setting learning rate to 1.6583130900471589e-06.\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0539 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 452: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0538 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 453: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0536 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 454: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0536 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 455: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0534 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 456: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0533 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 457: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0531 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 458: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0530 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 459: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0528 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 460: LearningRateScheduler setting learning rate to 1.6583130673097912e-06.\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0527 - lr: 1.6583e-06\n",
            "\n",
            "Epoch 461: LearningRateScheduler setting learning rate to 1.4924817605788121e-06.\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0526 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 462: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0525 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 463: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0524 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 464: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0522 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 465: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0521 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 466: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0520 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 467: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0519 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 468: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0518 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 469: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0516 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 470: LearningRateScheduler setting learning rate to 1.492481715104077e-06.\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0515 - lr: 1.4925e-06\n",
            "\n",
            "Epoch 471: LearningRateScheduler setting learning rate to 1.3432335435936694e-06.\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0514 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 472: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0513 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 473: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0512 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 474: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0510 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 475: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0509 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 476: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0509 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 477: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0507 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 478: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0506 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 479: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0505 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 480: LearningRateScheduler setting learning rate to 1.3432335208563018e-06.\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0504 - lr: 1.3432e-06\n",
            "\n",
            "Epoch 481: LearningRateScheduler setting learning rate to 1.2089101687706717e-06.\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0503 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 482: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0502 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 483: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0501 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 484: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0500 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 485: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0499 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 486: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0498 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 487: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0497 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 488: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0497 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 489: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0495 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 490: LearningRateScheduler setting learning rate to 1.2089101346646203e-06.\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0494 - lr: 1.2089e-06\n",
            "\n",
            "Epoch 491: LearningRateScheduler setting learning rate to 1.0880191211981583e-06.\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0493 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 492: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0493 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 493: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0491 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 494: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0491 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 495: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0490 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 496: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0489 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 497: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0488 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 498: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0487 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 499: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0486 - lr: 1.0880e-06\n",
            "\n",
            "Epoch 500: LearningRateScheduler setting learning rate to 1.0880190757234232e-06.\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0485 - lr: 1.0880e-06\n",
            "Skipping update for di_bits_cap_est_metrics, expected 4 values but got 100\n",
            "Skipping update for di_bits_cap_est_metrics, expected 4 values but got 100\n",
            "Skipping update for di_bits_cap_est_metrics, expected 4 values but got 100\n",
            "Monte Carlo Evaluation - Mean Loss: 1.8670549479793408e-06, Std Loss: 1.2806461446857043e-09\n",
            "MDP Evaluation - Average Reward: [[[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]]\n",
            "Monte Carlo Evaluation - Mean Loss: 1.867092464635789e-06, Std Loss: 2.2737367544323206e-13\n",
            "MDP Evaluation - Average Reward: [[[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]\n",
            "  [1.]]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x600 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nanZh87gIY1I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}